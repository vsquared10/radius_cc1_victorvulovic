{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf830
{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;\f1\fnil\fcharset0 HelveticaNeue-Bold;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid1\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\margl1440\margr1440
\hyphauto1\hyphfactor100
\deftab720
\pard\pardeftab720\sl480\slmult1\partightenfactor0

\f0\fs24 \cf2 \expnd0\expndtw0\kerning0
\up0 \nosupersub \ulnone \outl0\strokewidth0 \strokec2 Victor Vulovic\
Radius\
Coding Challenge 1 Results\
October 5, 2017\
\pard\pardeftab720\sl480\slmult1\qc\partightenfactor0
\cf2 \strokec2 Procedural Explanation and Data Findings\
\pard\pardeftab720\fi720\sl480\slmult1\partightenfactor0
\cf2 \strokec2 This initial coding challenge was presented to me by Samantha Rigan as a technical interview for the Data Scientist role. The purpose of this report is to elaborate on my thought process and to show the findings of the four calculation challenges: \
\pard\tx740\tx1132\pardeftab720\li392\fi327\sl480\slmult1\partightenfactor0
\ls1\ilvl0
\f1\b \cf2 \kerning1\expnd0\expndtw0 \up0 \nosupersub \ulnone \outl0\strokewidth0 {\listtext	1.	}\cf2 \expnd0\expndtw0\kerning0
\up0 \nosupersub \ulnone \outl0\strokewidth0 \strokec2 Fill Rate
\f0\b0 \cf2 \strokec2  - capture the initial rate of cells for which there is actually data\
\ls1\ilvl0
\f1\b \cf2 \kerning1\expnd0\expndtw0 \up0 \nosupersub \ulnone \outl0\strokewidth0 {\listtext	2.	}\cf2 \expnd0\expndtw0\kerning0
\up0 \nosupersub \ulnone \outl0\strokewidth0 \strokec2 True-Valued Fill Rate
\f0\b0 \cf2 \strokec2  - for the cells that are not empty, at what rate are those cells actually filled with relevant/useful data?\
\ls1\ilvl0
\f1\b \cf2 \kerning1\expnd0\expndtw0 \up0 \nosupersub \ulnone \outl0\strokewidth0 {\listtext	3.	}\cf2 \expnd0\expndtw0\kerning0
\up0 \nosupersub \ulnone \outl0\strokewidth0 \strokec2 Cardinality
\f0\b0 \cf2 \strokec2  - determine the total unique values for all the fields captured\
\ls1\ilvl0
\f1\b \cf2 \kerning1\expnd0\expndtw0 \up0 \nosupersub \ulnone \outl0\strokewidth0 {\listtext	4.	}\cf2 \expnd0\expndtw0\kerning0
\up0 \nosupersub \ulnone \outl0\strokewidth0 \strokec2 Something Interesting
\f0\b0 \cf2 \strokec2  - draw an interesting insight of my choosing from the data and discuss\
\pard\pardeftab720\fi720\sl480\slmult1\partightenfactor0
\cf2 \strokec2 Before diving into those results, I\'92ll describe a bit more about the data set I worked with and set up the context for my thought process. Then I\'92ll go ahead and elaborate on the points above and wrap up with some final thoughts.\
The data provided to me was a list of 1M businesses delivered by external providers and gave some basic descriptive fields about those businesses: name, location, revenue, etc. My initial goal was to get a sense of the quality of the data that I would be working with\'97 are there mistakes in the entries? How large is the data set? What are some typical occurrences to look out for?\
The first task of finding Fill Rate was relatively straight forward, and I wanted to include both raw counts along with proportions to give a better feel for the meaning of those ratios as they can often be misleading when given alone. For example, having 1/2 and 50/100 both equate to a ratio of 0.5, so it\'92s helpful to see raw performance juxtaposed with generalized performance. This raw count also proved to be helpful for comparison with the other calculations made later.\
True-Valued Fill Rate was a much more involved task and certainly the most challenging of the four tasks. I decided to use regular expressions to filter out the irrelevant entries in the fields. In other words, I took a more manual approach to examine what kind of patterns in each field could be expected to be \'93normal\'94 or \'93relevant\'94. For example, the \'93zipcode\'94 column has an obvious pattern: relevant entries will strictly be 5 digits long. Addresses, on the other hand, have much more variance in terms of legitimacy. For example, an address could be as simple as \'931 Sample Rd\'94, as complex as \'938707 New Yorkshire Blvd Ste. # 305-A\'94, and everything in between. Being able to correctly identify relevant addresses was a much more complicated task and required more extensive experimentation for the best pattern.\
We obviously want to maximize the amount of useful/relevant data that we gather, so we are incentivized to capture as much as possible. However, getting too much noise will lead to inaccuracy later down the road when we build a predictive model. Therefore, I assumed that entries of \'930\'94 for some numerical fields, such as revenue, were irrelevant. This is because the dominating pattern led me to believe that any business with zero revenue would not be one Radius is interested in (irrelevant), and there was a lower bound category that captured any company earning less than a minimum threshold.\
As expected, the True Value Fill Rate is lower than regular Fill Rate. It\'92s no stretch to expect that mistakes will be made in data entry or fields will just be left blank. I was interested to find that the field with the biggest negative impact was the \'93phone\'94 field. This makes intuitive sense since many businesses either don\'92t want flood themselves with phone calls and/or rely more solely upon internet-based communication channels.\
Cardinality was also a straight-forward task to achieve. The purpose of this statistic is to understand the diversity of the dataset; how many unique values exist in each field? The cardinality I calculated was on the original dataset, before the regular expressions were applied for true value counts. Directions for this were unclear, but if cardinality is necessary for the relevant values, I\'92d be happy to provide the result.\
Lastly, for an interesting fact, I wanted to see if there was a hotbed for billionaires in any of the states in particular. Where do all the billion dollar businesses reside? As one might expect, California is in the lead. Many companies in the billion dollar range are tech companies, which mainly are born in Silicon Valley. Add to the mix the entertainment industry in Los Angeles, and it\'92s no surprise that California is in the lead. Also, the financial services industry has had a strangle-hold on the most profitable companies list for several decades, so New York and Illinois (Chicago) are higher on the list. What is surprising, however, is the huge proportion of billion dollar businesses in Texas and Florida. My initial intuition is that Texas can still rely on oil, but Florida is still surprising\'97 perhaps many real estate, healthcare, or insurance companies are based there?\
Moving forward, it would be interesting to see how these values have changed over time. It\'92s possible that we could create a time-series forecasting model to predict which new companies are destined for greatness based on location, current revenue, current headcount, and time-in-business.
\f1\b \cf2 \strokec2 \
}